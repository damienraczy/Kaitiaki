paths:
  data_raw: "data/raw"
  data_processed: "data/processed"
  bm25_index: "data/processed/bm25_index.pkl"
  bm25_meta:  "data/processed/bm25_meta.json"

qdrant:
  # url: "http://localhost:6333"
  index: "kaitiaki_dev"
  host: "127.0.0.1"
  port: 6333

embedding:
  # soit un modèle local Sentence-Transformers
  model: "sentence-transformers/all-MiniLM-L6-v2"
  device: "cpu"  # "mps" possible sur Mac
  # embedding: "all-minilm-l6-v2"

reranker:     # cross-encoder pour reranking
  model: "mixedbread-ai/mxbai-rerank-xsmall-v1"
  # model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  # model: "BAAI/bge-reranker-base"
  top_k: 5
  
llm:
  api_key: "OLLAMA_API_KEY"                       # placeholder si requis
  # base_url: "http://localhost:11434"
  # model: "qwen2.5:7b-instruct"            # ou mistral:7b-instruct, etc.
  temperature: 0.2
  max_tokens: 4000
  base_url: "https://ollama.com/api"
  model: "gpt-oss:20b"            # ou mistral:7b-instruct, etc.
  
# retrieval:
#   top_k_dense: 20
#   top_k_bm25: 20
#   fusion: "rrf"
#   rerank_top_k: 25

retrieval:
  top_k_dense: 40   # Augmenté de 20 à 40
  top_k_bm25: 40    # Augmenté de 20 à 40
  fusion: "rrf"
  rerank_top_k: 25  # Maintenu à 25 pour ne pas surcharger le reranker


ui:
  default_days: 180

ingest:
  input_dir: "data/in"       # dossier d’entrée des textes/JSONL
  batch_size: 64
  bm25_index_path: "data/bm25_index"

runtime:
  top_k_dense: 20
  top_k_bm25: 20
  rrf_k: 60
  max_ctx_docs: 8
