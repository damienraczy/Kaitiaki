### Étape 1 : Améliorer la pertinence de la recherche (Cœur du RAG)

C'est l'étape la plus cruciale. Votre système fonctionne, il faut maintenant qu'il soit performant.

1.  **Activer et optimiser le Reranker** : Dans votre `pipeline.py`, vous chargez un `CrossEncoder` mais il n'est pas utilisé dans la fonction `hybrid_search` finale. L'intégration de ce reranker est la méthode la plus efficace pour améliorer la précision :
    * Après la fusion (RRF), passez les documents fusionnés à travers le `reranker.predict()` pour réordonner les résultats en fonction de leur pertinence sémantique par rapport à la question.
    * Utilisez votre script d'évaluation pour mesurer l'impact (le `recall@k` devrait augmenter).

2.  **Mettre en œuvre une véritable recherche hybride** : Actuellement, votre recherche lexicale (`_bm25_search`) est fonctionnelle mais les résultats ne sont pas pleinement intégrés pour récupérer le contenu des documents depuis Qdrant.
    * Assurez-vous que la logique de `hybrid_search` récupère bien le contenu textuel complet des documents identifiés par la recherche BM25. Votre code actuel contient déjà des filtres pour cela, il faut les finaliser.
    * Expérimentez avec les poids de la fusion RRF pour équilibrer l'importance de la recherche par mots-clés (BM25) et de la recherche sémantique (vecteurs).

3.  **Affiner la stratégie de "chunking"** : La qualité des réponses dépend énormément de la manière dont les documents sont découpés.
    * Analysez les scripts dans le dossier `kaitiaki/ingest/`. La taille des chunks est-elle optimale ? Y a-t-il un chevauchement (`overlap`) entre les chunks pour ne pas perdre le contexte ?
    * Envisagez des stratégies de découpage plus intelligentes, par exemple en respectant les paragraphes ou les sections des documents.

### Étape 2 : Améliorer l'expérience utilisateur et les fonctionnalités

1.  **Mettre en place le streaming des réponses** : Pour que l'application paraisse plus réactive, modifiez le `llm_client.py` et le `server.py` pour gérer les réponses en streaming. L'utilisateur verra la réponse s'afficher mot par mot, ce qui est standard pour les chatbots.
    * FastAPI supporte très bien cela avec les `StreamingResponse`.
    * Le JavaScript dans `index.html` devra être adapté pour gérer les `Server-Sent Events` (SSE).

2.  **Activer le filtrage par source** : Votre interface utilisateur pourrait permettre de sélectionner des documents ou des types de sources spécifiques pour la recherche.
    * Ajoutez un champ de filtrage dans le schéma `Query` de `schemas.py`.
    * Passez ce filtre à votre `pipeline.py` pour qu'il l'applique lors de la recherche dans Qdrant.

### Étape 3 : Industrialisation et robustesse

1.  **Ajouter des tests unitaires** : Pour éviter de casser des fonctionnalités existantes lors de futurs développements, il est essentiel d'ajouter des tests.
    * Créez un dossier `tests/` à la racine.
    * Utilisez `pytest` pour écrire des tests simples pour vos fonctions critiques, notamment `fusion.py` et `pipeline.py`.

2.  **Containeriser l'application avec Docker** : Pour simplifier le déploiement et garantir que l'environnement est cohérent, créez un `Dockerfile` pour votre application FastAPI et un fichier `docker-compose.yml` pour lancer Kaitiaki et Qdrant avec une seule commande.

3.  **Mettre en place une CI/CD simple** : Utilisez les "GitHub Actions" pour automatiser le lancement de vos tests à chaque fois que vous poussez du code sur le dépôt. Cela garantira la qualité et la non-régression de votre base de code.

En commençant par l'amélioration de la pertinence, vous vous assurez que le cœur de votre produit apporte une réelle valeur ajoutée. Ensuite, l'amélioration de l'expérience utilisateur et l'industrialisation rendront votre projet plus mature et plus facile à maintenir.